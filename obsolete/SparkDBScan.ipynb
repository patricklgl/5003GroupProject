{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/msbd5003/.local/lib/python3.5/site-packages/IPython/utils/py3compat.py:188 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5796b8bfe42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    334\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 336\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/msbd5003/.local/lib/python3.5/site-packages/IPython/utils/py3compat.py:188 "
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " UNTAGGED = 0\n",
    "TAGGED = 1\n",
    "INIT_TAG = -1\n",
    "CORE = 1\n",
    "UNCLASSIFIED = 0\n",
    "BOUNDARY = 0\n",
    "CORE = 1\n",
    "NOISE = -1\n",
    "TRADITIONAL = \"traditional\"\n",
    "ADVANCED = \"advanced\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(dataset,partition_shape,eps):\n",
    "    df=dataset.toDF().toPandas()\n",
    "    true_tags=[]\n",
    "    point_x=[]\n",
    "    point_y=[]\n",
    "    partition_num=partition_shape[0]*partition_shape[1]\n",
    "    for i in range(0, df.shape[0]):\n",
    "        point_x.append(float(df.iloc[i,0]))\n",
    "        point_y.append(float(df.iloc[i,1]))\n",
    "        true_tags.append(float(df.iloc[i,2]))\n",
    "    border_x=np.linspace(min(point_x),max(point_x),num=partition_shape[0]+1, endpoint=True)\n",
    "    border_y=np.linspace(min(point_y),max(point_y),num=partition_shape[1]+1, endpoint=True)\n",
    "    \n",
    "    lower_bound_list = []\n",
    "    upper_bound_list = []\n",
    "    \n",
    "    for y_low in border_y[:-1]:\n",
    "        for x_low in border_x[:-1]:\n",
    "            lower_bound_list.append([x_low,y_low])\n",
    "\n",
    "    lower_bound = []\n",
    "    for lower in lower_bound_list:\n",
    "        lower_bound.append([lower[0]-eps, lower[1]-eps])\n",
    "\n",
    "    for y_up in border_y[1:]:\n",
    "        for x_up in border_x[1:]:\n",
    "            upper_bound_list.append([x_up,y_up])\n",
    "\n",
    "    upper_bound = []\n",
    "    for upper in upper_bound_list:\n",
    "        upper_bound.append([upper[0]+eps, upper[1]+eps])\n",
    "        \n",
    "    partition_info={}\n",
    "    for i in range(0,len(lower_bound)):\n",
    "        partition_info[i]=[]\n",
    "    for p_id in range(0,df.shape[0]):     # index of point in dataset\n",
    "        for part_id in range(0,len(lower_bound)):\n",
    "            if (point_x[p_id]>lower_bound[part_id][0]) \\\n",
    "            and (point_y[p_id]>lower_bound[part_id][1])\\\n",
    "            and (point_x[p_id]<upper_bound[part_id][0])\\\n",
    "            and (point_y[p_id]<upper_bound[part_id][1]):\n",
    "                partition_info[part_id].append((p_id,(point_x[p_id],point_y[p_id])))\n",
    "            else:\n",
    "                continue\n",
    "    res = sc.parallelize(list(partition_info.items()),partition_num)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate the neighbours of point p\n",
    "def get_neighbours(dataset, pid, eps, num_p, method=ADVANCED):\n",
    "    neighbours = []\n",
    "    point_array = np.array(dataset)\n",
    "    if method == TRADITIONAL:\n",
    "        for nid in range(num_p):\n",
    "            distance = np.linalg.norm(point_array[pid] - point_array[nid])\n",
    "            if distance < eps:\n",
    "                neighbours.append(nid)\n",
    "        return neighbours\n",
    "\n",
    "    elif method == ADVANCED:\n",
    "        s = 1.25 * eps * math.sqrt(2)  ##2.5*（1/根号2）\n",
    "        distance = eps + 1\n",
    "        for nid in range(num_p):\n",
    "            if (point_array[pid][0] - s) <= point_array[nid][0] and point_array[nid][0] <= (point_array[pid][0] + s):\n",
    "                if (point_array[pid][1] - s) <= point_array[nid][1] and point_array[nid][1] <= (point_array[pid][1] + s):\n",
    "                    distance = np.linalg.norm(point_array[pid] - point_array[nid])\n",
    "\n",
    "            if distance < eps:\n",
    "                neighbours.append(nid)\n",
    "        return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(rdd_list, eps, minpts, method =TRADITIONAL):\n",
    "    # data_list = [pid for pid in range(num_p)]\n",
    "    point_id_list = []\n",
    "    dataset = []\n",
    "    for point in rdd_list:\n",
    "        point_id_list.append(point[0])\n",
    "        dataset.append(point[1])\n",
    "    num_p = len(point_id_list)  # num_p represents the number of points\n",
    "    tag_list = [UNCLASSIFIED] * num_p\n",
    "    core_list = [BOUNDARY] * num_p\n",
    "    noise_list = []\n",
    "    cluster_id = 1\n",
    "    for pid in range(num_p):\n",
    "        if tag_list[pid] == UNCLASSIFIED:\n",
    "            neighbour = get_neighbours(dataset, pid, eps, num_p, method)\n",
    "            if len(neighbour) < minpts:\n",
    "                noise_list.append(pid)\n",
    "            else:\n",
    "                tag_list[pid] = cluster_id\n",
    "                for nid in neighbour:\n",
    "                    if tag_list[nid] == UNCLASSIFIED:\n",
    "                        tag_list[nid] = cluster_id\n",
    "\n",
    "                    if nid in noise_list:\n",
    "                        noise_list.remove(nid)\n",
    "\n",
    "                while len(neighbour) > 0:\n",
    "                    cid = neighbour[0]  # current point id\n",
    "                    sub_neighbour = get_neighbours(dataset, cid, eps, num_p, method)\n",
    "                    if len(sub_neighbour) >= minpts:\n",
    "                        for sid in range(len(sub_neighbour)):\n",
    "                            if sub_neighbour[sid] in noise_list:\n",
    "                                noise_list.remove(sub_neighbour[sid])\n",
    "                            if tag_list[sub_neighbour[sid]] == UNCLASSIFIED:\n",
    "                                neighbour.append(sub_neighbour[sid])\n",
    "                                tag_list[sub_neighbour[sid]] = cluster_id\n",
    "                    elif len(sub_neighbour) < minpts:\n",
    "                        core_list[cid] = CORE\n",
    "\n",
    "                    neighbour = neighbour[1:]\n",
    "                cluster_id += 1\n",
    "\n",
    "    for nid in noise_list:\n",
    "        tag_list[nid] = NOISE\n",
    "\n",
    "    return (tag_list, core_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(partition_dict, core_list):\n",
    "    max_point_tags = 0\n",
    "    tagged_point_id = []\n",
    "    tagged_partition_id = []\n",
    "    status = []\n",
    "    for partition_key, partition_value in partition_dict.items():\n",
    "        max_tags = max_point_tags\n",
    "        for point_key, point_value in partition_value.items():\n",
    "            if point_value[\"tags\"] != NOISE:\n",
    "                point_value[\"tags\"] += max_point_tags  # make different partition has different tags.\n",
    "            if point_value[\"tags\"] > max_tags:\n",
    "                max_tags = point_value[\"tags\"]  # update max_point_tags.\n",
    "            point_value[\"is_tagged\"] = UNTAGGED\n",
    "            point_value[\"final_tags\"] = INIT_TAG\n",
    "        max_point_tags = max_tags\n",
    "\n",
    "    for partition_key, partition_value in partition_dict.items():\n",
    "        for point_key, point_value in partition_value.items():\n",
    "#             if point_value[\"is_tagged\"] == UNTAGGED:\n",
    "            if point_key not in tagged_point_id:\n",
    "                point_value[\"is_tagged\"] = TAGGED\n",
    "                point_value[\"final_tags\"] = point_value[\"tags\"]\n",
    "                tagged_point_id.append(point_key)\n",
    "                tagged_partition_id.append(partition_key)\n",
    "\n",
    "            else: \n",
    "                position = tagged_point_id.index(point_key)\n",
    "                initial_tags = point_value[\"tags\"]\n",
    "                point_value[\"is_tagged\"] = TAGGED\n",
    "                if partition_dict[tagged_partition_id[position]][tagged_point_id[position]][\"final_tags\"] == NOISE:\n",
    "                    partition_dict[tagged_partition_id[position]][tagged_point_id[position]][\"final_tags\"] = initial_tags\n",
    "                else:\n",
    "                    point_value[\"final_tags\"] = partition_dict[tagged_partition_id[position]][tagged_point_id[position]][\"final_tags\"]\n",
    "                for point_key2, point_value2 in partition_value.items():\n",
    "                    if point_value2[\"tags\"] == initial_tags and initial_tags != NOISE:\n",
    "                        point_value2[\"final_tags\"] = partition_dict[tagged_partition_id[position]][tagged_point_id[position]][\"final_tags\"]\n",
    "                        if point_key2 not in tagged_point_id:\n",
    "                            tagged_point_id.append(point_key2)\n",
    "                            tagged_partition_id.append(partition_key)\n",
    "        \n",
    "\n",
    "    return partition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset, partition_shape, eps, minpoint):\n",
    "    partitioned_data = partition(dataset, partition_shape, eps)\n",
    "    partitioned_data_list = partitioned_data.collect()\n",
    "    # rdd = [(partition_id, [(point_id, (x, y)), ]), () ...]\n",
    "    cluster_results = partitioned_data.mapValues(lambda x: cluster(x, eps, minpoint, method =TRADITIONAL)).collect()\n",
    "    # cluster_result =[(partition_id , (tag_list , core_list)),...]\n",
    "    #print(cluster_results)\n",
    "    core_list = []\n",
    "    tag_list = []\n",
    "    for cluster_result in cluster_results:\n",
    "        tag_list += cluster_result[1][0]\n",
    "        core_list += (cluster_result[1][1])\n",
    "    partition_id_list = []\n",
    "    point_id_list = []\n",
    "    coordinate_list = []\n",
    "    for pp in partitioned_data_list:\n",
    "        point_id_tmp = []\n",
    "        coordinate_tmp = []\n",
    "        for p in pp[1]:\n",
    "            point_id_tmp.append(p[0])\n",
    "            coordinate_tmp.append(p[1])\n",
    "            partition_id_list.append(pp[0])\n",
    "        point_id_list += (point_id_tmp)\n",
    "        coordinate_list += (coordinate_tmp)\n",
    "    partition_point_core_tag = zip(partition_id_list , point_id_list , core_list , tag_list , coordinate_list)\n",
    "    partition_point_core_tag\n",
    "    # partition_point_core_tag = [(parition_id,point_id,iscore,tag),...]\n",
    "    partition_point_dict = {}\n",
    "    for pp in partitioned_data_list:\n",
    "        partition_point_dict[pp[0]] = {}\n",
    "    for key, value in partition_point_dict.items():\n",
    "        for i in partition_point_core_tag:\n",
    "            if key == i[0]:\n",
    "                value[i[1]] = {\"is_core\":i[2] , \"tags\":i[3] ,\"coordinate\":i[4] }\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return merge(partition_point_dict, core_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_shape = (2,2)\n",
    "eps = 0.55\n",
    "minpoint = 14\n",
    "\n",
    "filepath=\"R15.txt\"\n",
    "dataset = sc.textFile(filepath).map(lambda x: x.strip().split())\n",
    "\n",
    "start = time.clock()\n",
    "info_dict = main(dataset, partition_shape, eps, minpoint)\n",
    "end = time.clock()\n",
    "\n",
    "runtime = end - start\n",
    "print(\"Runtime is ：\", runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from  matplotlib.colors import  rgb2hex\n",
    "def plot_result(final_dict, style=\"ticks\"):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    label = []\n",
    "    for key, value in final_dict.items():\n",
    "        x_list.append(value[\"coordinate\"][0])\n",
    "        y_list.append(value[\"coordinate\"][1])\n",
    "        label.append(value[\"final_tags\"])\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df['x'] = x_list\n",
    "    df['y'] = y_list\n",
    "    df['label'] = label\n",
    " \n",
    "    sns.set_style(style)\n",
    "    f, axes = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "    sns.scatterplot('x', 'y', data=df, ax=axes[0], palette='Blues_d')\n",
    "    sns.scatterplot('x', 'y', data=df, hue='label', ax=axes[1])\n",
    "\n",
    "    plt.title('DBSCAN Result')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "\n",
    "    plt.gca().get_legend().set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {} \n",
    "for key, value in info_dict.items():\n",
    "    final_dict = dict(final_dict.items()+value.items())\n",
    "\n",
    "plot_result(final_dict, \"ticks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
